# -*- coding: utf-8 -*-
"""HomeAssignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMOR_H75vzrv2gcsd2aeDWvU_9n9B6cj

# **Q1: NLP Preprocessing Pipeline**
"""

import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk

# Download only stopwords
nltk.download('stopwords')

def simple_tokenizer(text):
    # Tokenize using regex: split on spaces and remove punctuation
    tokens = re.findall(r'\b\w+\b', text)
    return tokens

def preprocess_nlp(sentence):
    # Step 1: Tokenization
    tokens = simple_tokenizer(sentence)
    print("Original Tokens:", tokens)

    # Step 2: Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    print("Tokens Without Stopwords:", filtered_tokens)

    # Step 3: Stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
    print("Stemmed Words:", stemmed_words)

# Run it
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
preprocess_nlp(sentence)

"""# Q2: Named Entity Recognition with SpaCy"""

# Install spaCy (run once)
!pip install -U spacy

# Download English language model
!python -m spacy download en_core_web_sm

import spacy

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Input sentence
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

# Process the sentence
doc = nlp(sentence)

# Print each entity's text, label, and character position
for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

"""# Q3: Scaled Dot-Product Attention (NumPy)"""

import numpy as np

# Define softmax
def softmax(x):
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e_x / e_x.sum(axis=-1, keepdims=True)

# Inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Step 1: Dot product of Q and K.T
scores = np.dot(Q, K.T)

# Step 2: Scale by sqrt(d), where d is the key dimension
d_k = Q.shape[-1]
scaled_scores = scores / np.sqrt(d_k)

# Step 3: Softmax to get attention weights
attention_weights = softmax(scaled_scores)

# Step 4: Multiply attention weights with V
output = np.dot(attention_weights, V)

# Print outputs
print("Attention Weights:\n", attention_weights)
print("\nOutput Matrix:\n", output)

"""#  Q4: Sentiment Analysis using HuggingFace Transformers"""

# Install transformers library
!pip install transformers --quiet

# Import pipeline
from transformers import pipeline

# Load sentiment-analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Analyze
result = sentiment_pipeline(sentence)[0]

# Print result
print(f"Sentiment: {result['label']}")
print(f"Confidence Score: {round(result['score'], 4)}")

